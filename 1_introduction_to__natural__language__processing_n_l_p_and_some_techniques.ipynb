{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohsal2026/github.com/blob/main/1_introduction_to__natural__language__processing_n_l_p_and_some_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "588dfd41",
      "metadata": {
        "id": "588dfd41"
      },
      "source": [
        "ðŸŽ¥ Recommended Video: [Natural Language Processing: Crash Course AI](https://www.youtube.com/watch?v=oi0JXuL19TA)\n",
        "\n",
        "ðŸŽ¥ Recommended Video: [What is NLP? Learn Natural Language Processing in Artificial Intelligence\n",
        "](https://www.youtube.com/watch?v=QwBaFEeUUMA)\n",
        "\n",
        "\n",
        "# What is Natural Language Processing (NLP)?\n",
        "\n",
        "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It involves enabling computers to understand, interpret, and generate human language in a meaningful way. Applications include language translation, sentiment analysis, chatbots, and more.\n",
        "\n",
        "\n",
        "## Natural Language Processing (NLP) Applications\n",
        "\n",
        "### Ontology in NLP\n",
        "Ontology refers to the way we organize and represent concepts in a structured manner so that they can be easily manipulated by computer programs. It defines concepts and the relationships between them. For example, a triple like `(\"python\", \"language\", \"is-a\")` represents the relationship that Python is a language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Touring Popular NLP Libraries and Picking Up NLP Basics\n",
        "\n",
        "In this lecture, we will explore essential Python libraries for Natural Language Processing (NLP) and learn fundamental NLP concepts. We will cover libraries like **NLTK**, **spaCy**, **Gensim**, and **TextBlob**, along with key NLP tasks such as tokenization, part-of-speech tagging, named entity recognition, stemming, and lemmatization.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Installing Famous NLP Libraries\n",
        "\n",
        "### 1.1 NLTK (Natural Language Toolkit)\n",
        "NLTK is one of the most popular libraries for NLP. It is widely used for educational and industrial purposes.\n",
        "\n",
        "**Installation**:\n",
        "```bash\n",
        "# Using pip\n",
        "sudo pip install -U nltk\n",
        "\n",
        "# Using conda\n",
        "conda install nltk\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2 spaCy\n",
        "spaCy is a powerful and memory-optimized NLP library written in Cython. It uses state-of-the-art algorithms for tasks like tagging and named entity recognition.\n",
        "\n",
        "**Installation**:\n",
        "```bash\n",
        "# Using pip\n",
        "pip install -U spacy\n",
        "\n",
        "# Using conda\n",
        "conda install -c conda-forge spacy\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3 Gensim\n",
        "Gensim is a library designed for topic modeling and similarity retrieval. It is highly efficient and scalable.\n",
        "\n",
        "**Installation**:\n",
        "```bash\n",
        "# Using pip\n",
        "pip install --upgrade gensim\n",
        "\n",
        "# Using conda\n",
        "conda install -c conda-forge gensim\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1.4 TextBlob\n",
        "TextBlob is built on top of NLTK and provides easy-to-use interfaces for common NLP tasks like sentiment analysis, translation, and spell checking.\n",
        "\n",
        "**Installation**:\n",
        "```bash\n",
        "# Using pip\n",
        "pip install -U textblob\n",
        "\n",
        "# Using conda\n",
        "conda install -c conda-forge textblob\n",
        "```"
      ],
      "metadata": {
        "id": "8hKvkZ6IgMwt"
      },
      "id": "8hKvkZ6IgMwt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exploring NLTK Corpora\n",
        "\n",
        "NLTK provides over 100 corpora (text datasets) for NLP tasks. Some popular corpora include:\n",
        "- **Gutenberg Corpus**: Literary works from Project Gutenberg.\n",
        "- **Reuters Corpus**: News articles for text classification.\n",
        "- **Movie Reviews Corpus**: Sentiment analysis dataset.\n",
        "- **WordNet**: Lexical database of English words."
      ],
      "metadata": {
        "id": "fx2eB3acgZdU"
      },
      "id": "fx2eB3acgZdU"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b38f2c31",
      "metadata": {
        "id": "b38f2c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1b1234-3257-4634-b73c-bf57ac21c65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n",
            "7944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('names')\n",
        "\n",
        "from nltk.corpus import names\n",
        "\n",
        "# Print the first 10 names\n",
        "print(names.words()[:10])\n",
        "\n",
        "# Total number of names\n",
        "print(len(names.words()))  # Output: 7944"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. NLP Tasks and Techniques\n",
        "\n",
        "### 3.1 Text Vectorization\n",
        "\n",
        "Text vectorization is the process of converting text into numerical representations.\n",
        "\n",
        "#### Key Uses of Text Vectorization\n",
        "- Preparing text for machine learning models.\n",
        "- Enabling similarity comparisons between texts."
      ],
      "metadata": {
        "id": "zHsdc1KGg3TE"
      },
      "id": "zHsdc1KGg3TE"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Import Data Files from Google Drive\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "def read_gd(sharingurl):\n",
        "    file_id = sharingurl.split('/')[-2]\n",
        "    download_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
        "    url = requests.get(download_url).text\n",
        "    csv_raw = StringIO(url)\n",
        "    return csv_raw\n",
        "\n",
        "url = \"https://drive.google.com/file/d/1FgIpdZaw7ell0zSvhbtpB9Ex4Ck3Mbcw/view?usp=sharing\"\n",
        "gdd = read_gd(url)\n",
        "\n",
        "documents = gdd.read().splitlines()\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATZvLiJDg4cY",
        "outputId": "351dfc66-169c-4bdd-ac9b-b96ffb3080a9"
      },
      "id": "ATZvLiJDg4cY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.47569405 0.         0.56343915\n",
            "  0.         0.         0.35202427 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.40763366 0.\n",
            "  0.40763366]\n",
            " [0.         0.         0.         0.70261292 0.         0.\n",
            "  0.         0.         0.25997466 0.         0.         0.\n",
            "  0.         0.         0.5074392  0.         0.30104295 0.\n",
            "  0.30104295]\n",
            " [0.         0.50211386 0.         0.         0.         0.\n",
            "  0.         0.         0.25724635 0.         0.         0.50211386\n",
            "  0.         0.         0.         0.         0.29788364 0.50211386\n",
            "  0.29788364]\n",
            " [0.         0.         0.         0.47569405 0.         0.56343915\n",
            "  0.         0.         0.35202427 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.40763366 0.\n",
            "  0.40763366]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.4501536  0.4501536  0.23062572 0.         0.         0.\n",
            "  0.4501536  0.4501536  0.         0.36913239 0.         0.\n",
            "  0.        ]\n",
            " [0.4198708  0.         0.4198708  0.         0.4198708  0.\n",
            "  0.         0.         0.         0.4198708  0.4198708  0.\n",
            "  0.         0.         0.         0.34430007 0.         0.\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows how important each word is to each document, with higher values indicating more important words. Words that appear in many documents (like \"the\", \"is\") will get lower weights, while words specific to particular documents will get higher weights."
      ],
      "metadata": {
        "id": "3G4hurR9hG8q"
      },
      "id": "3G4hurR9hG8q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Text Classification\n",
        "\n",
        "Text classification involves categorizing text into predefined groups.\n",
        "\n",
        "#### Key Uses of Text Classification\n",
        "- Spam detection.\n",
        "- Sentiment analysis.\n",
        "- Topic categorization."
      ],
      "metadata": {
        "id": "6pjiucyqhQSc"
      },
      "id": "6pjiucyqhQSc"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Sample documents and labels (equal length!)\n",
        "documents = [\n",
        "    \"The team won the championship\",               # sports\n",
        "    \"Soccer match ended in a draw\",               # sports\n",
        "    \"New smartphone released with advanced AI\",    # tech\n",
        "    \"Programming languages are evolving fast\",     # tech\n",
        "    \"Basketball playoffs start next week\",        # sports\n",
        "    \"The new laptop has a 16-hour battery life\",  # tech\n",
        "    \"Olympic athletes train rigorously\",          # sports\n",
        "    \"Quantum computing breakthroughs announced\"   # tech\n",
        "]\n",
        "labels = [\"sports\", \"sports\", \"tech\", \"tech\", \"sports\", \"tech\", \"sports\", \"tech\"]\n",
        "\n",
        "# Vectorize Text (TF-IDF)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, labels, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqP-LXTFhKRM",
        "outputId": "b2459182-24ff-4da0-9e0f-c38ee7d7395e"
      },
      "id": "bqP-LXTFhKRM",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['16' 'advanced' 'ai' 'announced' 'are' 'athletes' 'basketball' 'battery'\n",
            " 'breakthroughs' 'championship' 'computing' 'draw' 'ended' 'evolving'\n",
            " 'fast' 'has' 'hour' 'in' 'languages' 'laptop' 'life' 'match' 'new' 'next'\n",
            " 'olympic' 'playoffs' 'programming' 'quantum' 'released' 'rigorously'\n",
            " 'smartphone' 'soccer' 'start' 'team' 'the' 'train' 'week' 'with' 'won']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      sports       0.50      1.00      0.67         1\n",
            "        tech       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Tokenization\n",
        "Tokenization is the process of breaking text into smaller units like words or sentences."
      ],
      "metadata": {
        "id": "f793XrVZhjj_"
      },
      "id": "f793XrVZhjj_"
    },
    {
      "cell_type": "code",
      "source": [
        "## Word Tokenization with NLTK:\n",
        "import nltk\n",
        "try:\n",
        "    nltk.download('punkt_tab')\n",
        "except:\n",
        "    nltk.download('punkt')  # Fallback to punkt\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"I am reading a book. It is Python Machine Learning By Example, 4th edition.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2Jmu8LVhkjz",
        "outputId": "bdc845b4-e6ea-4c1c-e08d-c056d89cabd0"
      },
      "id": "F2Jmu8LVhkjz",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'reading', 'a', 'book', '.', 'It', 'is', 'Python', 'Machine', 'Learning', 'By', 'Example', ',', '4th', 'edition', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization with NLTK\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brcxUZWchp9Y",
        "outputId": "0540913b-02d1-44a4-8286-4e21ff68c9e0"
      },
      "id": "brcxUZWchp9Y",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I am reading a book.', 'It is Python Machine Learning By Example, 4th edition.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization with spaCy\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I have been to U.K. and U.S.A.\")\n",
        "print([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARLBIekHj1XK",
        "outputId": "6305a97d-7a62-4286-ee78-e7cee85e2b7a"
      },
      "id": "ARLBIekHj1XK",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'have', 'been', 'to', 'U.K.', 'and', 'U.S.A.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Part-of-Speech (PoS) Tagging\n",
        "PoS tagging assigns grammatical categories (e.g., noun, verb) to words in a sentence."
      ],
      "metadata": {
        "id": "YkfAXlCKkIek"
      },
      "id": "YkfAXlCKkIek"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "try:\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "except:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(\"I am reading a book.\")\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0kYufDfkJd7",
        "outputId": "ac173176-6c98-4c7f-e713-995512a2b9df"
      },
      "id": "B0kYufDfkJd7",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('reading', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PoS Tagging with spaCy\n",
        "\n",
        "doc = nlp(\"I have been to U.K. and U.S.A.\")\n",
        "print([(token.text, token.pos_) for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p__jW-uBlI4j",
        "outputId": "1ad656a2-44d0-438c-8520-18b74f334a84"
      },
      "id": "p__jW-uBlI4j",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRON'), ('have', 'AUX'), ('been', 'AUX'), ('to', 'ADP'), ('U.K.', 'PROPN'), ('and', 'CCONJ'), ('U.S.A.', 'PROPN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Named Entity Recognition (NER)\n",
        "NER identifies and classifies named entities like persons, organizations, and locations."
      ],
      "metadata": {
        "id": "JqM4qdvRliGA"
      },
      "id": "JqM4qdvRliGA"
    },
    {
      "cell_type": "code",
      "source": [
        "# NER with spaCy\n",
        "\n",
        "doc = nlp(\"The book written by Hayden Liu in 2024 was sold at $30 in America.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YymU6dPBllcf",
        "outputId": "d9cdb397-3c96-4b3a-9e33-8043d12ff2ee"
      },
      "id": "YymU6dPBllcf",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hayden Liu', 'PERSON'), ('2024', 'DATE'), ('30', 'MONEY'), ('America', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Stemming and Lemmatization\n",
        "Stemming reduces words to their root form, while lemmatization converts words to their base or dictionary form."
      ],
      "metadata": {
        "id": "e-S-GiOml2AT"
      },
      "id": "e-S-GiOml2AT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming with NLTK\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "print(stemmer.stem(\"machines\"))\n",
        "print(stemmer.stem(\"learning\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_FJ_3_nl0XL",
        "outputId": "345b0d54-9dcf-4374-ad20-96355d2f0c09"
      },
      "id": "6_FJ_3_nl0XL",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machin\n",
            "learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization with NLTK\n",
        "\n",
        "import nltk\n",
        "# Dependencies\n",
        "nltk.download('wordnet')  # Download the WordNet corpus\n",
        "nltk.download('omw-1.4')  # Open Multilingual WordNet\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"machines\"))\n",
        "print(lemmatizer.lemmatize(\"learning\", pos=\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdEJbCtfmFKn",
        "outputId": "2010f040-b378-425a-f08c-3aaeb9a87ee9"
      },
      "id": "zdEJbCtfmFKn",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine\n",
            "learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Topic Modelling\n",
        "Topic modeling involves uncovering abstract topics within a collection of documents. Essentially, it helps in identifying patterns and themes without needing any prior labels or annotations."
      ],
      "metadata": {
        "id": "68G3GtK1mrfS"
      },
      "id": "68G3GtK1mrfS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modelling Using Gensym\n",
        "\n",
        "# !pip uninstall -y numpy gensim scipy\n",
        "# !pip install --no-cache-dir numpy==1.23.5 gensim==4.3.2 scipy==1.9.3\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\"\n",
        "]\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stop_words]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# LDA model\n",
        "lda_model = gensim.models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=3,\n",
        "    random_state=42,\n",
        "    passes=15,\n",
        "    alpha='auto'\n",
        ")\n",
        "\n",
        "# Print topics\n",
        "for idx, topic in lda_model.print_topics(num_words=4):\n",
        "    print(f\"Topic {idx}: {topic}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "2HyO8UTWmqmb",
        "outputId": "329e033e-9b96-4016-f442-a773cb049c73"
      },
      "id": "2HyO8UTWmqmb",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-1209399415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "4nWv_GnwpZ-Z"
      },
      "id": "4nWv_GnwpZ-Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.8 Word Embeddings (BERT)\n",
        "\n",
        "Word embeddings are dense vector representations of words that capture semantic relationships. Among various models, BERT (Bidirectional Encoder Representations from Transformers) has become a significant advancement in NLP.\n",
        "\n",
        "#### What is BERT?\n",
        "BERT is a deep learning model designed by Google to improve understanding of the context of words in a sentence. Unlike traditional embeddings, BERT uses a bidirectional approach, meaning it considers both the left and right context of a word simultaneously.\n",
        "\n",
        "#### Key Uses of BERT\n",
        "- **Text Classification**: Sentiment analysis, spam detection, etc.\n",
        "- **Question Answering**: Extracting precise answers from text.\n",
        "- **Named Entity Recognition**: Identifying entities in text.\n",
        "- **Machine Translation**: Translating languages with high accuracy."
      ],
      "metadata": {
        "id": "lJ_pumgPrqtt"
      },
      "id": "lJ_pumgPrqtt"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy torch transformers\n",
        "!pip install --no-cache-dir numpy==1.21.2 torch transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Kdl2c86Zrp09"
      },
      "id": "Kdl2c86Zrp09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize text\n",
        "inputs = tokenizer(\"I love natural language processing!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Extract embeddings\n",
        "print(outputs.last_hidden_state.shape)  # Should print: torch.Size([1, 7, 768])"
      ],
      "metadata": {
        "id": "rClspe-g3KY8"
      },
      "id": "rClspe-g3KY8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.9 Language Modeling\n",
        "\n",
        "Language modeling predicts the next word in a sentence or sequence of words.\n",
        "\n",
        "#### Key Uses of Language Modeling\n",
        "- Text generation.\n",
        "- Autocomplete systems.\n",
        "- Language translation."
      ],
      "metadata": {
        "id": "q817Qudf7QI2"
      },
      "id": "q817Qudf7QI2"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load GPT-2\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Generate text\n",
        "inputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs['input_ids'], max_length=50, num_return_sequences=1)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "LLv0ykwR7RIn"
      },
      "id": "LLv0ykwR7RIn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.10 Machine Translation\n",
        "\n",
        "Machine Translation involves translating text from one language to another.\n",
        "\n",
        "#### Key Uses of Machine Translation\n",
        "- Cross-language communication.\n",
        "- Global content localization."
      ],
      "metadata": {
        "id": "bXUeqKTU9Iym"
      },
      "id": "bXUeqKTU9Iym"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# Load Marian model for English to French\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Translate text\n",
        "text = \"How are you?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "g5628iSL9KIb"
      },
      "id": "g5628iSL9KIb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.11 Large Language Models (LLMs) and Attention Mechanisms\n",
        "\n",
        "LLMs are trained on massive datasets to understand and generate human-like text. Attention mechanisms help focus on relevant parts of the input sequence.\n",
        "\n",
        "#### Key Uses of LLMs\n",
        "- Summarization.\n",
        "- Question answering.\n",
        "- Creative writing."
      ],
      "metadata": {
        "id": "fxAr8u1M9Y-Q"
      },
      "id": "fxAr8u1M9Y-Q"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load a pre-trained model (T5)\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Example usage\n",
        "# text = \"summarize: Machine learning enables computers to learn from data.\"\n",
        "\n",
        "text = \"\"\"summarize: Climate change is one of the most pressing issues of our time.\n",
        "Rising global temperatures, caused primarily by human activities such as burning fossil fuels\n",
        "and deforestation, are leading to severe consequences like extreme weather events, melting\n",
        "ice caps, and rising sea levels. Scientists warn that without immediate action, these effects\n",
        "will become irreversible. Governments worldwide are implementing policies to reduce carbon\n",
        "emissions, while individuals can contribute by adopting sustainable practices like using\n",
        "renewable energy, reducing waste, and supporting eco-friendly initiatives. The time to act\n",
        "is nowâ€”delaying further will only exacerbate the crisis.\"\"\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs['input_ids'], max_length=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "YAArIdRO9aAt"
      },
      "id": "YAArIdRO9aAt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.12 Chatbots\n",
        "Chatbots simulate conversation with users by understanding input and generating meaningful responses.\n",
        "\n",
        "#### Key Uses of Chatbots\n",
        "-Customer support automation.\n",
        "-Personalized assistance.\n",
        "-Interactive learning."
      ],
      "metadata": {
        "id": "Nhnr76KX_q3y"
      },
      "id": "Nhnr76KX_q3y"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "\n",
        "# Load Blenderbot\n",
        "model_name = \"facebook/blenderbot-400M-distill\"\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Chatbot interaction\n",
        "text = \"Hello, how can I help you?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs['input_ids'], max_length=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "hsHe5cpL_H3M"
      },
      "id": "hsHe5cpL_H3M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    print(\"Bot: Hi! I'm your friendly AI. Type 'quit' to end the chat.\")\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        # Format input and generate response\n",
        "        inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=100,\n",
        "            num_beams=5,          # Better quality responses\n",
        "            temperature=0.7,      # More creative answers\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Print response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"Bot: {response}\")\n",
        "\n",
        "# Start chatting\n",
        "chat()"
      ],
      "metadata": {
        "id": "Lx-178jQAgtB"
      },
      "id": "Lx-178jQAgtB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Real-World NLP Applications\n",
        "\n",
        "#### 4.1 Sentiment Analysis\n",
        "Sentiment analysis involves determining the sentiment (e.g., positive, negative, or neutral) expressed in a text. For example:\n",
        "- **Binary Classification**: Positive or negative sentiment.\n",
        "- **Multiclass Classification**: Positive, neutral, or negative sentiment.\n",
        "\n",
        "**Use Case**: News sentiment analysis can provide valuable signals for stock market trading.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.2 News Topic Classification\n",
        "News topic classification assigns categories (e.g., technology, sports, religion) to news articles. Categories may or may not be mutually exclusive. For example:\n",
        "- An article about the Olympic Games could be labeled as both **sports** and **politics** if there is political involvement.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.3 Named Entity Recognition (NER)\n",
        "NER identifies and classifies named entities in text, such as:\n",
        "- **Persons**: Elon Musk\n",
        "- **Organizations**: SpaceX\n",
        "- **Locations**: California\n",
        "- **Dates**: 2020\n",
        "- **Quantities**: 9 meters\n",
        "\n",
        "**Example**:\n",
        "> \"SpaceX[Organization], a California[Location]-based company founded by Elon Musk[Person], announced a 9[Quantity]-meter-diameter launch vehicle for 2020[Date].\"\n",
        "\n",
        "---\n",
        "\n",
        "####  Other Key NLP Applications\n",
        "\n",
        "#### 4.4 Language Translation\n",
        "NLP powers machine translation systems like **Google Translate** and **Microsoft Translator**, enabling real-time translation between languages.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.5 Speech Recognition\n",
        "NLP converts spoken language into written text. Examples include virtual assistants like **Siri**, **Alexa**, and **Google Assistant**.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.6 Text Summarization\n",
        "NLP can generate concise summaries of lengthy texts, aiding in information retrieval and content curation.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.7 Language Generation\n",
        "NLP models like **Generative Pre-trained Transformers (GPTs)** can generate human-like text, including creative writing, poetry, and dialogue.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.8 Information Retrieval\n",
        "NLP helps retrieve relevant information from unstructured data (e.g., web pages, documents). Search engines use NLP to understand user queries and fetch appropriate results.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.9 Chatbots and Virtual Assistants\n",
        "NLP powers interactive systems like chatbots and virtual assistants, enabling them to answer queries, assist with tasks, and guide users.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "A0vVoROXBexx"
      },
      "id": "A0vVoROXBexx"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}